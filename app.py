import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

# Load the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")

# Load the psychology dataset
ds = load_dataset("ilyass31/jkhedri-psychology-llama2-dataset")

# Streamlit UI
st.title("Llama-3.2-3B-Instruct Text Generation with Psychology Dataset")
st.write("Enter a query below to get a response generated by Llama-3.2-3B-Instruct.")

# Display sample from the dataset
if 'train' in ds:
    st.write("Example from the dataset:")
    st.json(ds['train'][0])  # Display first entry for reference

# User input
input_text = st.text_area("Ask the model a question:", "Who are you?")

# If the user has input a question, generate the response
if input_text.strip():
    st.write("Generating response...")

    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors="pt")

    # Generate the model output
    outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Display the response
    st.subheader("Response:")
    st.write(generated_text)
else:
    st.info("Please enter a question to get a response.")
